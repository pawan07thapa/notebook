{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('digits.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Now we split the image to 5000 cells, each 20x20 size\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Make it into a Numpy array. It size will be (50,100,20,20)\n",
    "x = np.array(cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training set and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we prepare feature set\n",
    "x = x[:,:].reshape(-1,400).astype(np.float32) # Size = (5000,400)\n",
    "\n",
    "# Now we prepare label set\n",
    "y=np.ones((5000,1),dtype=np.uint8)\n",
    "for i in range(10):\n",
    "    y[500*i:500*(i+1)]*=i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model and determining accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:904: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.92\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10,10), random_state=1)\n",
    "\n",
    "clf.fit(x_train,y_train) \n",
    "\n",
    "result=clf.predict(x_test).reshape(1250,1)\n",
    "\n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==y_test\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = correct*100.0/result.size\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.64\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train) \n",
    "\n",
    "result=knn.predict(x_test).reshape(1250,1)\n",
    "\n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==y_test\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = correct*100.0/result.size\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(knn, open('trained_ocr.pic', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('trained_ocr.pic', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image is: (128, 139)\n",
      "shape of smaller image is: (20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x845f0651d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted it as : [4]\n"
     ]
    }
   ],
   "source": [
    "img=cv2.imread('test_number.png',0)\n",
    "print('shape of image is: {}'.format(img.shape))\n",
    "\n",
    "small=cv2.resize(img,(20,20))\n",
    "small^=255\n",
    "# ret,small=cv2.threshold(small,127,255,cv2.THRESH_BINARY_INV)\n",
    "print('shape of smaller image is: {}'.format(small.shape))\n",
    "\n",
    "plt.imshow(small,cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "test=small.reshape(1,400)\n",
    "print('Model predicted it as : {}'.format(loaded_model.predict(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live video feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[5]\n",
      "[6]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    small=cv2.resize(gray,(20,20))\n",
    "    ret,small=cv2.threshold(small,127,255,cv2.THRESH_BINARY_INV)\n",
    "    cv2.imshow(' ',small)\n",
    "\n",
    "    if cv2.waitKey(500)&0xFF!=255:\n",
    "        break\n",
    "    print(loaded_model.predict(small.reshape(1,400)))\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
