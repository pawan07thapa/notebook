{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Introduction](#1.-Introduction)\n",
    "* [2. Types of model](#2.-Types-of-model)\n",
    "* [3. Cross validation set](#3.-Cross-validation-set)\n",
    "  * [k- Fold Cross Validation](#k--Fold-Cross-Validation)\n",
    "  * [But how do we choose k?](#But-how-do-we-choose-k?)\n",
    "* [4. How do I measure the performance of my classification model?](#4.-How-do-I-measure-the-performance-of-my-classification-model?)\n",
    "  * [Confusion Matrix](#Confusion-Matrix)\n",
    "  * [Log Loss or Logarithmic Loss or Cross Entropy Loss](#Log-Loss-or-Logarithmic-Loss-or-Cross-Entropy-Loss)\n",
    "  * [Receiver Operating characteristics(ROC) and Area under ROC Curve(AUC)](#Receiver-Operating-characteristics%28ROC%29-and-Area-under-ROC-Curve%28AUC%29)\n",
    "* [5. How do I measure the performance of my regression model?](#5.-How-do-I-measure-the-performance-of-my-regression-model?)\n",
    "  * [Root Mean Square Error (RMSE)](#Root-Mean-Square-Error-%28RMSE%29)\n",
    "  * [$R^2$](#$R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Rule 1:**  \n",
    "Never use the testing data for tarining purposes. Use it at very end to evaluate performance of the model.\n",
    "\n",
    "A good fitting model is one where the difference between the actual or observed values and predicted values for the selected model is small and unbiased for train, validation and test data sets. Predictive Modeling works on constructive feedback principle. You build a model. Get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspects of evaluation metrics is their capability to discriminate among model results.\n",
    "\n",
    "Simply, building a predictive model is not your motive. But, creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check accuracy of the model prior to computing predicted values. In our industry, we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Types of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Machine learning models are of two types:\n",
    "\n",
    "* **Regression** — The output is a continuous variable (eg. Predict Housing Prices).\n",
    "* **Classification** — The output is a discrete variable (eg. Cat vs Dog). Classification could be binary or a multi class classification.\n",
    "\n",
    "The evaluation metrics used in each of these models are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross validation is used in Machine Learning to estimate the skill of a ML model on unseen data.\n",
    "* It generally results in less biased estimate of model and every data point is used in test set once.\n",
    "* It has a parameter (k) which specifies number of groups the training dataset is split into.\n",
    "* A model can have error due to bias (underfitting) and error due to variance (overfitting). So, multiple models are trained with various complexities (For example: regression model of order 1,2,3,4,.. can be trained to fit the dataset).\n",
    "* To find which model performs better, we divide the dataset into training set, cross validation set and test set.\n",
    "* We train the model on training set and test it on cross validation set, with various complexities of model.\n",
    "* We plot the training set error and cross validation set error for all models.\n",
    "* CV set error is minimum for optimum model and after model is trained, it is tested on test set.\n",
    "* **In summary, we use train set to train multiple model, cross validation set to select best model and test set to evaluate overall performance of the model.**\n",
    "\n",
    "![variation of train set error and cross validation error with complexity](cross validation error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative side of this simple cross validation set is that we loose a good amount of data from training the model. Hence, the model is very high bias. And this won’t give best estimate for the coefficients.\n",
    "\n",
    "## k- Fold Cross Validation\n",
    "Now, we will try to visualize how does a k-fold validation work.\n",
    "![k-fold cross validation](k_fold.png \"k-fold cross validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 7-fold cross validation.\n",
    "\n",
    "Here’s what goes on behind the scene : \n",
    "1. We divide the entire population into 7 equal samples. \n",
    "* Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). \n",
    "* Then, at the second iteration we train the model with a different sample held as validation. \n",
    "* In 7 iterations, we have basically built model on each sample and held each of them as validation. \n",
    "* This is a way to reduce the selection bias and reduce the variance in prediction power. \n",
    "* Once we have all the 7 models, we take average of the error terms to find which of the models is best.\n",
    "\n",
    "## But how do we choose k?\n",
    "This is the tricky part. We have a trade off to choose k.  \n",
    "For a small k, we have a higher selection bias but low variance in the performances.  \n",
    "For a large k, we have a small selection bias but high variance in the performances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How do I measure the performance of my classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Binary classification we expect an output of 0 or 1. The output is a predictive score which conveys the probability of the output to be either a 0 or a 1. Typically if the score is the above a certain threshold value then we set the output to 1 else the output will be 0. This threshold value is usually selected as 0.5 but can vary.\n",
    "\n",
    ">To evaluate the performance of a binary classification model we use Confusion matrix, Log Loss/Cross Entropy, or AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is an N X N matrix, where N is the number of classes being predicted. It is a table that tells us how many actual values and predicted values exists for different classes that the model will predict. Also referred as **Error matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![confusion matrix](confusion_matrix.png)\n",
    "![confusion matrix](confusion_matrix1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **True Positive :** When prediction is **True** (correct) and actual class was **positive**.\n",
    "* **True Negative :** When prediction is **True** (correct) and actual class was **negative**.\n",
    "* **False Positive :** When prediction is **False** (incorrect) and actual class was **positive**. This is Type 1 error.\n",
    "* **False Negative :** When prediction is **False** (incorrect) and actual class was **negative**. Also called as Type 2 error.\n",
    "* **Accuracy :** The proportion of the total number of predictions that were correct.\n",
    "\n",
    "\n",
    "$$\n",
    "Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "\n",
    "* **Positive Predictive Value or Precision :** The proportion of positive cases that were correctly identified.\n",
    "\n",
    "\n",
    "$$\n",
    "Precision=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "\n",
    "* **Negative Predictive Value :** The proportion of negative cases that were correctly identified.\n",
    "* **Sensitivity or Recall:** The proportion of actual positive cases which are correctly identified.\n",
    "\n",
    "\n",
    "$$\n",
    "Recall=\\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "\n",
    "* **Specificity :** The proportion of actual negative cases which are correctly identified.\n",
    "\n",
    "\n",
    "$$\n",
    "Specificity=\\frac{TN}{FP+TN}\n",
    "$$\n",
    "\n",
    "* **F1 score :** It uses precision and recall score of the model to create 1 score which defines the performance of the model. It is harmonic mean of Precision and Recall. Range for F1 score is between 0 and 1 and a larger value means better prediction.\n",
    "\n",
    "$$\n",
    "\\text{F1 score}=\\frac{2*Precision*Recall}{Precision+Recall}\n",
    "$$\n",
    "\n",
    "* **$F_\\beta$ score :** Smaller the value of $\\beta$, more weight towards precision and when $\\beta$ value is large, then more weight towards recall. F1 score is $F_\\beta$ score with $\\beta=1$.\n",
    "\n",
    "$$\n",
    "F_\\beta=\\frac{(1+\\beta^2)*Precision*Recall}{(\\beta^2*Precision)+Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss or Logarithmic Loss or Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss should be close to 0 for a good binary classification model. Log loss increases as the predicted values diverges from the actual value. A perfect model will have a log loss of 0. It is calculated as:\n",
    "$$\n",
    "\\text{Log Loss}=-(y*\\log{p}+(1-y)*\\log{(1-p)})\n",
    "$$\n",
    "In the above equation  \n",
    "$y$ : is the actual output  \n",
    "$p$ : predicted value  \n",
    "\n",
    "**Accuracy** is the proportion of correct predictions for positive and negative class over the total number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating characteristics(ROC) and Area under ROC Curve(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve is a graph showing the performance of a classification model at different classification threshold. ROC curve is a plot between true positive rate(TPR) and false positive rate(FPR) at different classification thresholds so various thresholds results in different true positive rate and false positive rates.\n",
    "\n",
    "**True Positive Rate(TPR)** is the proportion of the positive data points correctly predicted to all the actual positive data points.\n",
    "\n",
    "**False Positive Rate(FPR)** is the proportion of the negative data points falsely predicted as positive to all the actual negative data points.\n",
    "\n",
    "$$\n",
    "\\text{Sensitivity / Recall / True Positive Rate}=\\frac{\\text{True Positive (TP)}}{\\text{Actual Positive (TP+FN)}}\\\\\n",
    "\\text{False Positive Rate}=\\frac{\\text{False Positive (FP)}}{\\text{Actual Negative (TN+FP)}}\n",
    "$$\n",
    "\n",
    "When we lower the classification threshold say from 0.5 to 0.3. In that scenario we will classify more data points as positive thus increasing the true positive and false positives.\n",
    "\n",
    "**To compute the points in an ROC curve we can evaluate the model at different classification threshold but that would be inefficient and so we use AUC or Area under ROC Curve.**\n",
    "\n",
    "![AUC](auc.png)\n",
    "\n",
    "The dashed line is random classifier from which you can expect as many true positives as false positives.\n",
    "\n",
    "we can think of AUC as the probability where model ranks a randomly positive example more highly than a randomly negative example.\n",
    "\n",
    "AUC ranges between 0 and 1.\n",
    "\n",
    "A value of 0 means 100% prediction of the model is incorrect. A value of 1 means that 100% prediction of the model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How do I measure the performance of my regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A few statistical tools like coefficient of determination also called as $R^2$, Adjusted $R^2$ and Root mean square Error -RMSE are commonly used to evaluate the performance of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square Error (RMSE)\n",
    "\n",
    "RMSE shows the variation between the predicted and the actual value. Since the difference between predicted and actual values can be positive or negative . To offset that difference we take the square of the difference between predicted and actual value.\n",
    "\n",
    "1. Find the difference between predicted and actual value for every observation and square the value and add them.\n",
    "* divide the sum by number of observations.\n",
    "* Take the square root of the value from step 2.\n",
    "\n",
    "$$\n",
    "RMSE=\\sqrt{\\frac{\\sum_{i=1}^N(\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also called as coefficient of determination.\n",
    "\n",
    "R² gives us a measure of how well the actual outcomes are replicated by the model or the regression line. This is based on the total variation of prediction explained by the model.\n",
    "\n",
    "$$\n",
    "R^2=\\frac{\\text{Variance explained by the model}}{\\text{Total variance}}\n",
    "$$\n",
    "\n",
    "$R^2$ is always between 0 and 1 or between 0% to 100%. A value of 1 means that the model explains all the variation in predicted variable around its mean.\n",
    "\n",
    "**Sum square of errors(SSE)** or Residuals, how far did we predict a value when compared to the actual value.\n",
    "$$\n",
    "SSE = \\text{Actual value} - \\text{Predicted value}\n",
    "$$\n",
    "\n",
    "**Sum square of total (SST)**, how far is the actual value when compared to the mean value.\n",
    "$$\n",
    "SST = \\text{Actual value} - \\text{Mean value}\n",
    "$$\n",
    "\n",
    "**Sum square of Regression(SSR)**, how far is the actual value when compared to the mean value.\n",
    "$$\n",
    "SSR = \\text{Predicted value} - \\text{mean value}\n",
    "$$\n",
    "\n",
    "So, $R^2$ is defined as,\n",
    "$$\n",
    "R^2=1-\\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "If the error in prediction is low then SSE will be low and R² will be close to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
