{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this post is to give the reader detailed understanding of Principal Component Analysis with the necessary mathematical proofs. \n",
    "\n",
    "In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the data and find various patterns in it or use it to train some machine learning models. \n",
    "\n",
    "As the dimensions of data increases, the difficulty to visualize it and perform computations on it also increases. So, how to reduce the dimensions of a data-\n",
    "* Remove the redundant dimensions\n",
    "* Only keep the most important dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. What is PCA?](#1.-What-is-PCA?)\n",
    "* [2. Some mathematical terms](#2.-Some-mathematical-terms)\n",
    "  * [2.1 Variance](#2.1-Variance)\n",
    "  * [2.2 Covariance](#2.2-Covariance)\n",
    "* [3. How PCA works?](#3.-How-PCA-works?)\n",
    "  * [3.1 Loading the dataset](#3.1-Loading-the-dataset)\n",
    "  * [3.2 Calculating the covariance matrix](#3.2-Calculating-the-covariance-matrix)\n",
    "    * [3.2.1 Calculating covariance matrix for Iris dataset](#3.2.1-Calculating-covariance-matrix-for-Iris-dataset)\n",
    "  * [3.3 Calculate eigen vectors and corresponding eigen values.](#3.3-Calculate-eigen-vectors-and-corresponding-eigen-values.)\n",
    "  * [3.4 Sort the eigen vectors according to their eigen values in decreasing order.](#3.4-Sort-the-eigen-vectors-according-to-their-eigen-values-in-decreasing-order.)\n",
    "  * [3.5 Choose first k Eigen vectors and that will be the new k dimensions.](#3.5-Choose-first-k-Eigen-vectors-and-that-will-be-the-new-k-dimensions.)\n",
    "  * [3.6 Transform the original n dimensional data points into k dimensions.](#3.6-Transform-the-original-n-dimensional-data-points-into-k-dimensions.)\n",
    "* [4. Some derivations](#4.-Some-derivations)\n",
    "  * [Theorem-1](#Theorem-1)\n",
    "  * [Theorem-2](#Theorem-2)\n",
    "  * [4.1 Inverse Transform](#4.1-Inverse-Transform)\n",
    "  * [4.1.1 Finding approx. original dataset](#4.1.1-Finding-approx.-original-dataset)\n",
    "* [5. Reference](#5.-Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along them. It means more important principle\n",
    "axis occurs first. (more important = more variance/more spread out data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some mathematical terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance :** It is a measure of the variability or it simply measures how spread the data set is. Mathematically, it is the average squared deviation from the mean score. We use the following formula to compute variance var(x).\n",
    "$$ var (x) = \\frac{\\sum(x_i - \\mu_x)^2}{N} $$\n",
    "where $\\mu_x$ is the mean of the set of values $x_i$ and $N$ is total number of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance :** It is a measure of the extent to which corresponding elements from two sets of ordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the covariance of x and y.\n",
    "$$ cov(x,y)=\\frac{\\sum(x_i-\\mu_x)(y_i-\\mu_y)}{N} $$\n",
    "where $\\mu_x$ and $\\mu_y$ are means values of $x_i$ and $y_i$ respectively, each having $N$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to observe the covariance is how interrelated two data sets are.\n",
    "\n",
    "![](covariance_image.png)\n",
    "\n",
    "Positive covariance means X and Y are positively related i.e. as X increases Y also increases. Negative covariance depicts the exact opposite relation. However zero covariance means X and Y are not related.\n",
    "\n",
    "Now lets think about the requirement of data analysis.\n",
    "Since we try to find the patterns among the data sets so we want the data to be spread out across each dimension. Also, we want the dimensions to be independent. Such that if data has high covariance when represented in some n number of dimensions then we replace those dimensions with linear combination of those n dimensions. Now that data will only be dependent on linear combination of those related n dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How PCA works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does PCA work -\n",
    "\n",
    "1. Load the dataset\n",
    "1. Calculate the covariance matrix `cov_x` of data points.\n",
    "1. Calculate eigen vectors and corresponding eigen values.\n",
    "1. Sort the eigen vectors according to their eigen values in decreasing order.\n",
    "1. Choose first k eigen vectors and that will be the new k dimensions.\n",
    "1. Transform the original n dimensional data points into k dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use **Iris dataset**, to visualise PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1    2    3    4            5\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the dataset\n",
    "df=pd.read_csv('iris_data.csv',names=[1,2,3,4,5])\n",
    "# Storing the shape of the dataset\n",
    "m,n=df.shape\n",
    "# 'x' contain all the feature values\n",
    "x=df[[1,2,3,4]].values\n",
    "# Creating the label map\n",
    "label_map={ind:val for ind,val in zip(range(3),df[5].unique())}\n",
    "# 'y' contains all the label mapping to corresponding numbers\n",
    "y=np.zeros((m,1),dtype=np.uint8)\n",
    "for ind,val in label_map.items():\n",
    "    y[df[5]==val]=ind\n",
    "# Displaying the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Calculating the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have the knowledge of variance and covariance, Lets look into what a **Covariance matrix** is.\n",
    "\n",
    "Let $C_x$ covariance matrix of some data set in 4 dimensions a,b,c,d.   \n",
    "$V_a$ : variance along dimension a.  \n",
    "$C_{a,b}$ : Covariance along dimension a and b.\n",
    "\n",
    "Then $C_x$ is defined as:\n",
    "$$ C_x= \n",
    "\\begin{bmatrix}\n",
    "V_a &C_{a,b} &C_{a,c} &C_{a,d} \\\\\n",
    "C_{b,a} &V_b &C_{b,c} &C_{b,d} \\\\\n",
    "C_{c,a} &C_{c,b} &V_c &C_{c,d} \\\\\n",
    "C_{d,a} &C_{d,b} &C_{d,c} &V_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we have a matrix X of (m,n) dimension such that it holds m data points of n dimensions, then covariance matrix can be calculated as:\n",
    "\n",
    "$$ C_x= \\frac{1}{N}(X-\\bar{X})(X-\\bar{X})^T $$\n",
    "\n",
    "It is important to note that the covariance matrix contains -\n",
    "* variance of dimensions as the main diagonal elements.\n",
    "* covariance of dimensions as the off diagonal elements.\n",
    "* covariance matrix is symmetric.\n",
    "\n",
    "As, we discussed earlier we want the data to be spread out i.e. it should have high variance along dimensions. Also we want to remove correlated dimensions i.e. covariance among the dimensions should be zero (they should be linearly independent). Therefore, our covariance matrix should have -\n",
    "* large numbers as the main diagonal elements.\n",
    "* zero values as the off diagonal elements.\n",
    "We call it a diagonal matrix.\n",
    "\n",
    "So, we have to transform the original data points such that their covariance is a diagonal matrix. The process of transforming a matrix to diagonal matrix is called **diagonalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Calculating covariance matrix for Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of feature 0 is -4.736951571734001e-16\n",
      "mean of feature 1 is -1.5158245029548805e-15\n",
      "mean of feature 2 is 4.7369515717340015e-17\n",
      "mean of feature 3 is -9.473903143468003e-17\n",
      "Covariance martix is:\n",
      "[[ 1.46816528 -0.30665611  0.60066444  1.30302466]\n",
      " [-0.30665611  5.35473323 -0.55335261 -1.08472247]\n",
      " [ 0.60066444 -0.55335261  0.32337083  0.71978638]\n",
      " [ 1.30302466 -1.08472247  0.71978638  1.72851418]]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "m,n=x.shape\n",
    "for i in range(n):\n",
    "    mean=(1/m)*x[:,i].sum()\n",
    "    variance=(1/m)*((x[:,i]-mean)**2).sum()\n",
    "    x[:,i]=(x[:,i]-mean)/variance\n",
    "# After feature scaling the dataset is 0 mean\n",
    "for i in range(n):\n",
    "    print('mean of feature {} is {}'.format(i,(1/m)*x[:,i].sum()))\n",
    "# Calculating the covariance matrix\n",
    "cov=(1/m)*np.dot(x.T,x)\n",
    "print('Covariance martix is:\\n{}'.format(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to understand what I mean by projection error. Suppose we have to transform a 2 dimensional representation of data points to a one dimensional representation. So we will basically try to find a straight line and project data points on them. (A straight line is one dimensional). There are many possibilities to select the straight line. Lets see two such possibilities -\n",
    "\n",
    "![](finding_axis1.png)\n",
    "![](finding_axis2.png)\n",
    "\n",
    "If you see the red lines (connecting the projection of blue points on magenta line) i.e. the perpendicular distance of each data point from the straight line is the projection error. Sum of the error of all data points will be the total projection error.\n",
    "\n",
    "Clearly, Second choice of straight line is better because -\n",
    "* The projection error is less than that in the first case.\n",
    "* Newly projected red points are more widely spread out than the first case. i.e. more variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculate eigen vectors and corresponding eigen values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate eigenvalues and eigenvectors we use `np.linalg.eig()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen values are:\n",
      " [ 5.88698949  2.74141181  0.23702845  0.00935377]\n",
      "Eigen vectors are:\n",
      " [[-0.17921219  0.64902732  0.72356116  0.15200583]\n",
      " [ 0.9171719   0.38189383 -0.10759758 -0.03708883]\n",
      " [-0.15219391  0.25445129 -0.06577843 -0.95276689]\n",
      " [-0.32173859  0.60677441 -0.67864218  0.26029578]]\n"
     ]
    }
   ],
   "source": [
    "eig_val,eig_vect=np.linalg.eig(cov)\n",
    "print('Eigen values are:\\n {}'.format(eig_val))\n",
    "print('Eigen vectors are:\\n {}'.format(eig_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sort the eigen vectors according to their eigen values in decreasing order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above list of eigen values we see that they are already sorted in decreasing order of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Choose first k Eigen vectors and that will be the new k dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 eigen values comparable, and remaining 2 eigen values have smaller values and thus can be neglected. So, we select first 2 eigenvectors corresponding to those large eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Transform the original n dimensional data points into k dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of eigenvectors is the transformation matrix, with which we matrix multiply dataset to obtain transformed dataset and among the transformed dataset with n=4 features, we select first k=2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transformed dataset is: (150, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8049716d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed=np.dot(x,eig_vect)\n",
    "print('Shape of transformed dataset is: {}'.format(transformed.shape))\n",
    "# Plotting first k=2 features\n",
    "color={0:'r',1:'g',2:'b'}\n",
    "for ind,val in color.items():\n",
    "    data_pts=transformed[np.where(y==ind)[0]]\n",
    "    plt.scatter(data_pts[:,0],data_pts[:,1],c=val,label=label_map[ind])\n",
    "plt.legend()\n",
    "plt.xlabel('Principle component 1')\n",
    "plt.ylabel('Principle component 2')\n",
    "plt.title('PCA plotting of Iris dataset.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Some derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to transform the original data points such that the covariance matrix of transformed data points is a diagonal matrix. How to do that ?  \n",
    "$C_x=$covariance matrix of original dataset $X$.  \n",
    "$C_y=$covariance matrix of transformed dataset $Y$.  \n",
    "such that, $Y=PX$.  \n",
    "\n",
    "For simplicity, we discard the mean term and assume the data to be centered, i.e, $X=(X-\\bar{X})$\n",
    "So,\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "C_x=& \\frac{1}{N}XX^T \\\\\n",
    "C_y=& \\frac{1}{N}YY^T \\\\\n",
    "=& \\frac{1}{N}(PX)(PX)^T \\\\\n",
    "=& \\frac{1}{N}PXX^TP^T \\\\\n",
    "=& P(\\frac{1}{N}XX^T)P^T \\\\\n",
    "=& PC_xP^T \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Hereâ€™s the trick- If we find the matrix of eigen vectors of Cx and use that as P (P is used for transforming X to Y, see the image above) , then Cy (covariance of transformed points) will be a diagonal matrix. Hence Y will be the set of new/transformed data points.\n",
    "Now, if we want to transform points to k dimensions then we will select first k eigen vectors of the matrix Cx (sorted decreasingly according to eigen values) and form a matrix with them and use them as P.\n",
    "\n",
    ">So, if we have m dimensional original n data points then\n",
    "$$\n",
    "X : m*n \\\\\n",
    "P : k*m \\\\\n",
    "Y = PX : (k*m)(m*n) = (k*n)\n",
    "$$\n",
    "Hence, our new transformed matrix has n data points having k dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The inverse of an orthogonal matrix is its transpose.**\n",
    "\n",
    "**Proof:**\n",
    "For orthogonal matrix,\n",
    "$$\n",
    "A^TA=I \\\\\n",
    "A^{-1}=A^T \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let A be a real symmetric matrix and $\\lambda_1,\\lambda_2,\\dots,\\lambda_k$ be distinct eigenvalues of A. \n",
    "Let $u_i \\in R^n$ be non zero eigenvectors such that, $1\\leq i\\leq k$.   \n",
    "Then ${u_1,u_2,\\dots,u_k}$ forms an orthonormal set.**\n",
    "\n",
    "**Proof:**\n",
    "For $i \\neq j$, $1 \\leq i,j \\leq k$, since $A^T=A$, we have:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\lambda_i*u_iu_j^T=(\\lambda_i*u_i)u_j^T\n",
    "=(Au_i)u_j^T=u_i(A^Tu_j^T)=u_i(Au_j^T)=\\lambda_j*u_iu_j^T\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Since, $i \\neq j$, we have $\\lambda_i \\neq \\lambda_j$ and hence, $u_iu_j^T=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inverse Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform is done to retrive the approximated version of original dataset. Since, we know $Y=PX$ so we can find X axxording to $X=P^{-1}Y$. Given that P is a square matrix. Also since `P` is the transformation matrix consist of all eigenvectors, so it is orthogonal (according to [Theorem-2](#Theorem-2)). So inverse of $P$ is $P^{-1}=P^T$ (according to [Theorem-1](#Theorem-1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Finding approx. original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual dataset looks like:\n",
      "\n",
      "[[-1.09133619  2.38821102 -0.762724   -1.7262095 ]\n",
      " [-1.38496925 -0.28915559 -0.762724   -1.7262095 ]\n",
      " [-1.67860231  0.78179105 -0.79506108 -1.7262095 ]\n",
      " [-1.82541883  0.24631773 -0.73038691 -1.7262095 ]\n",
      " [-1.23815272  2.92368434 -0.762724   -1.7262095 ]]\n",
      "Approximated dataset looks like:\n",
      "\n",
      "[[-1.09133619  2.38821102 -0.762724   -1.7262095 ]\n",
      " [-1.38496925 -0.28915559 -0.762724   -1.7262095 ]\n",
      " [-1.67860231  0.78179105 -0.79506108 -1.7262095 ]\n",
      " [-1.82541883  0.24631773 -0.73038691 -1.7262095 ]\n",
      " [-1.23815272  2.92368434 -0.762724   -1.7262095 ]]\n"
     ]
    }
   ],
   "source": [
    "# Original dataset approx\n",
    "approx=np.dot(transformed,eig_vect.T)\n",
    "print('Actual dataset looks like:\\n')\n",
    "print(x[:5,:])\n",
    "print('Approximated dataset looks like:\\n')\n",
    "print(approx[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to PCA by medium.com](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
